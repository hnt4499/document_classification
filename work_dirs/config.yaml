data:
    train:
        paths:
            - data/processed/split_0.csv
            - data/processed/split_1.csv
            - data/processed/split_2.csv
            - data/processed/split_3.csv
            - data/processed/split_4.csv
            - data/processed/split_5.csv
            - data/processed/split_8.csv
            - data/processed/split_9.csv
        p_augmentation: 1.0
    val:
        paths:
            - data/processed/split_6.csv
    test:
        paths:
            - data/processed/split_7.csv
    mapping_path: data/processed/mapping.json
    max_word_count: 150
    min_word_count: 50

model:
    model_name_or_path: bert-base-uncased
    config_name: null
    tokenizer_name: null
    cache_dir: null

    model_class: BertForDBpediaDocumentClassification
    freeze_base_model: False
    fusion: max_pooling
    lambdas: [1, 1, 1]

training:
    work_dir: work_dirs/  # set to `null` to not save anything
    learning_rate: 5e-5
    weight_decay: 0.01
    lr_warmup: 0.1  # fraction of total number of iterations used to warm up training
    max_grad_norm: 1.0

    device: cuda:1
    batch_size: 32
    batch_size_multiplier: 1.2  # eval/train batch size ratio
    num_epochs: 100
    num_workers: 8
    debugging: false  # set to 'true' to perform only 10 iterations per epoch
    early_stopping: 2  # stop training when model is not improved over this number of epochs
    early_stopping_metrics: [l1_f1_macro, l2_f1_macro, l3_f1_macro]
